{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uhcX37ZAA03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " :( thats not good. maybe use the basic vdb instead\n",
            "Successfully connected to Pinecone index with integrated embeddings\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n#define Pinecone index name and dimension\\nPINECONE_INDEX_NAME = \"chatbot-memory\"\\nEMBEDDING_DIMENSION = 768  # standard dimension for many embedding models\\n\\n#create Pinecone index if it doesn\\'t exist\\ntry:\\n    # check if index already exists\\n    if PINECONE_INDEX_NAME not in pinecone.list_indexes():\\n        # create new index with proper schema\\n        pinecone.create_index(\\n            name=PINECONE_INDEX_NAME,\\n            dimension=EMBEDDING_DIMENSION,\\n            metric=\"cosine\",  # use cosine similarity for text embeddings\\n            spec=pinecone.Spec(\\n                serverless=pinecone.ServerlessSpec(\\n                    cloud=\"aws\",\\n                    region=\"us-east-1\"\\n                )\\n            )\\n        )\\n        print(f\"Created new Pinecone index \\'{PINECONE_INDEX_NAME}\\'\")\\n    else:\\n        print(f\"Using existing Pinecone index \\'{PINECONE_INDEX_NAME}\\'\")\\n\\n    # connect to the index\\n    index = pinecone.Index(PINECONE_INDEX_NAME)\\n    print(f\"Successfully connected to Pinecone index\")\\n\\nexcept Exception as e:\\n    print(f\"Error initializing Pinecone: {e}\")\\n    # fallback to simulated VDB if Pinecone fails\\n    index = None  \\n'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title Setup and Imports\n",
        "\n",
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Configure your API key \n",
        "APIKEY = \"AIzaSyA6bnQK33HYRspkrOi8-Q54bq4E4RgcHj4\" \n",
        "# It's recommended to store your API key securely, e.g., in environment variables\n",
        "# For this example, we'll assume it's directly set.\n",
        "# Replace \"YOUR_API_KEY\" with your actual Google API Key\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "genai.configure(api_key=APIKEY) \n",
        "\n",
        "# Initialize the Generative Model\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "# Chat history and truncation settings\n",
        "chat_history = []\n",
        "MAX_CHAT_HISTORY_LENGTH = 4 # Number of recent turns to keep in active memory, so # of messages by user + chatbot = 2 * chat_history_length (I think) \n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Configure the Pinecone API key \n",
        "pineconeAPIKEY = \"pcsk_4xPCuD_6WLyNse1TcP3qmHKhMPKNdRCSQwU8g9MvVVVorLvLjWVboM3CwA76YnNzu8yd4V\" \n",
        "\n",
        "#initialize pinecone client\n",
        "pc = Pinecone(api_key=pineconeAPIKEY)  \n",
        "PINECONE_INDEX_NAME = \"chatbot-memory-integrated\" \n",
        "if not pc.has_index(PINECONE_INDEX_NAME):\n",
        "    pc.create_index_for_model(\n",
        "        name = PINECONE_INDEX_NAME,      \n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\",\n",
        "        embed = { \n",
        "            \"model\": \"llama-text-embed-v2\", #Does this allow for auto embedding without needing an embedding function?? I think so.  \n",
        "            \"field_map\": {\"text\": \"message_text\"}\n",
        "        }\n",
        "    )\n",
        "    print(f\"Created new Pinecone index '{PINECONE_INDEX_NAME}' with integrated embedding model\")\n",
        "else:\n",
        "    print(f\" :( thats not good. maybe use the basic vdb instead\")  \n",
        "\n",
        "index = pc.Index(PINECONE_INDEX_NAME)\n",
        "print(f\"Successfully connected to Pinecone index with integrated embeddings\")\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#i dont think we need this part below. \n",
        "\n",
        "'''\n",
        "#define Pinecone index name and dimension\n",
        "PINECONE_INDEX_NAME = \"chatbot-memory\"\n",
        "EMBEDDING_DIMENSION = 768  # standard dimension for many embedding models\n",
        "\n",
        "#create Pinecone index if it doesn't exist\n",
        "try:\n",
        "    # check if index already exists\n",
        "    if PINECONE_INDEX_NAME not in pinecone.list_indexes():\n",
        "        # create new index with proper schema\n",
        "        pinecone.create_index(\n",
        "            name=PINECONE_INDEX_NAME,\n",
        "            dimension=EMBEDDING_DIMENSION,\n",
        "            metric=\"cosine\",  # use cosine similarity for text embeddings\n",
        "            spec=pinecone.Spec(\n",
        "                serverless=pinecone.ServerlessSpec(\n",
        "                    cloud=\"aws\",\n",
        "                    region=\"us-east-1\"\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        print(f\"Created new Pinecone index '{PINECONE_INDEX_NAME}'\")\n",
        "    else:\n",
        "        print(f\"Using existing Pinecone index '{PINECONE_INDEX_NAME}'\")\n",
        "    \n",
        "    # connect to the index\n",
        "    index = pinecone.Index(PINECONE_INDEX_NAME)\n",
        "    print(f\"Successfully connected to Pinecone index\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Pinecone: {e}\")\n",
        "    # fallback to simulated VDB if Pinecone fails\n",
        "    index = None  \n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using real Pinecone Vector Database with integrated embeddings\n",
            "Pinecone index is already built and optimized for similarity search.\n"
          ]
        }
      ],
      "source": [
        "# Pinecone Vector Database Class (replacing SimulatedVectorDB) \n",
        "class PineconeVectorDB:\n",
        "    def __init__(self, index): \n",
        "        self.index = index  # Real Pinecone index connection\n",
        "        self._is_built = True  # Pinecone indexes are always built\n",
        "        self.item_counter = 0  # Track item IDs\n",
        "\n",
        "    def add_item(self, item_id: int, embedding: list, text_content: str):\n",
        "        \"\"\"Adds an item with text content - Pinecone handles embedding generation.\"\"\"\n",
        "        # with integrated models, we send text directly: no need for embeddings\n",
        "        # Enhanced metadata for better retrieval \n",
        "        metadata = {\n",
        "            \"item_id\": str(item_id),\n",
        "            \"timestamp\": str(np.datetime64('now')), \n",
        "            \"message_type\": \"chat_message\",\n",
        "            \"text_content\": text_content,\n",
        "            \"word_count\": len(text_content.split()),  # For chunking optimization \n",
        "            \"keywords\": self._extract_keywords(text_content)  # For hybrid search  \n",
        "        }  \n",
        "        \n",
        "        # Upsert with text: Pinecone generates embeddings automatically\n",
        "        self.index.upsert(\n",
        "            vectors=[(str(item_id), {\"message_text\": text_content}, metadata)]\n",
        "        )   \n",
        "        \n",
        "        print(f\"Added item ID {item_id} to Pinecone VDB (Text: '{text_content[:30]}...')\")\n",
        "\n",
        "    def build(self, n_trees: int = 10): #I think a default value for n_trees is ok \n",
        "        #Pinecone indexes are automatically built: no manual build needed.\n",
        "        print(f\"Pinecone index is already built and optimized for similarity search.\")\n",
        "        self._is_built = True # We probably don't need the function I think.\n",
        "\n",
        "    def query(self, query_text: str, k: int = 1) -> list:  \n",
        "        #Real advanced semantic search using Pinecone's integrated model.\n",
        "        #Returns the k most semantically similar items hybrid search techniques. \n",
        "        \n",
        "        if not self._is_built:\n",
        "            print(\"Pinecone index is always ready for querying. We continue as needed.\")\n",
        "\n",
        "        try:  \n",
        "            # Query with text directly: Pinecone handles embedding generation; Query expansion \n",
        "            expanded_queries = self._expand_query(query_text)   \n",
        "            \n",
        "            # Hybrid search combining semantic and keyword matching\n",
        "            all_results = []\n",
        "            \n",
        "            for expanded_query in expanded_queries:\n",
        "                # Semantic search with expanded query\n",
        "                query_results = self.index.query(\n",
        "                    vector={\"message_text\": expanded_query},\n",
        "                    top_k=k * 2,  # Get more results for re-ranking\n",
        "                    include_metadata=True\n",
        "                )\n",
        "                \n",
        "                # Re-rank results based on multiple factors   \n",
        "                reranked_results = self._rerank_results(query_results.matches, query_text)\n",
        "                all_results.extend(reranked_results)\n",
        "            \n",
        "            # Deduplicate and select top k results\n",
        "            final_results = self._deduplicate_and_select_top(all_results, k)\n",
        "            \n",
        "            # Format results\n",
        "            retrieved_results = []\n",
        "            for result in final_results:\n",
        "                item_id = result['id']\n",
        "                text_content = result['text_content']\n",
        "                similarity_score = result['score']\n",
        "                retrieval_method = result.get('method', 'semantic')\n",
        "                retrieved_results.append(f\"Retrieved content (ID: {item_id}, Score: {similarity_score:.3f}, Method: {retrieval_method}): '{text_content}'\")\n",
        "            \n",
        "            return retrieved_results\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error querying Pinecone: {e}. Please try again.\")\n",
        "            return [] \n",
        "\n",
        "    # Query Expansion \n",
        "    def _expand_query(self, query_text: str) -> list: \n",
        "        #Expand query with synonyms and related terms for better retrieval.\n",
        "        expanded_queries = [query_text]  # Original query \n",
        "        \n",
        "        # Simple synonym expansion (when making this more advanced, use a proper thesaurus API which idk)   \n",
        "        synonyms = {   \n",
        "            \"what\": [\"tell me about\", \"explain\", \"describe\"],\n",
        "            \"how\": [\"explain how\", \"describe the process\", \"what is the method\"],\n",
        "            \"why\": [\"explain why\", \"what is the reason\", \"what causes\"],\n",
        "            \"when\": [\"at what time\", \"during what period\", \"what date\"],\n",
        "            \"where\": [\"in what location\", \"at what place\", \"which place\"],\n",
        "            \"who\": [\"which person\", \"what person\", \"tell me about\"]\n",
        "        }\n",
        "        \n",
        "        # Expand query with synonyms\n",
        "        words = query_text.lower().split()\n",
        "        for word in words:\n",
        "            if word in synonyms:\n",
        "                for synonym in synonyms[word]:\n",
        "                    expanded_query = query_text.lower().replace(word, synonym)\n",
        "                    if expanded_query not in expanded_queries:\n",
        "                        expanded_queries.append(expanded_query)\n",
        "        \n",
        "        # Add question variations \n",
        "        if query_text.endswith('?'):\n",
        "            # Remove question mark and add as statement\n",
        "            statement_query = query_text[:-1].strip()\n",
        "            if statement_query not in expanded_queries:\n",
        "                expanded_queries.append(statement_query)\n",
        "        \n",
        "        return expanded_queries[:3]  # Limit to 3 expanded queries\n",
        "\n",
        "    # Re-ranking Method. NEED HELP CUZ IDK IF ITS GOOD \n",
        "    def _rerank_results(self, matches, original_query: str) -> list:\n",
        "        #Re-rank results based on multiple relevance factors. \n",
        "        \n",
        "        reranked = []\n",
        "        \n",
        "        for match in matches:\n",
        "            score = match.score\n",
        "            text_content = match.metadata.get('text_content', '')\n",
        "            \n",
        "            # Boost score based on keyword overlap\n",
        "            keyword_boost = self._calculate_keyword_overlap(original_query, text_content)\n",
        "            \n",
        "            # Boost score based on recency (newer messages slightly preferred)\n",
        "            recency_boost = self._calculate_recency_boost(match.metadata.get('timestamp', ''))\n",
        "            \n",
        "            # Boost score based on content length (prefer meaningful responses)\n",
        "            length_boost = self._calculate_length_boost(text_content)\n",
        "            \n",
        "            # Combined score with weights\n",
        "            final_score = (score * 0.6 + keyword_boost * 0.2 + recency_boost * 0.1 + length_boost * 0.1)\n",
        "            \n",
        "            reranked.append({\n",
        "                'id': match.id,\n",
        "                'text_content': text_content,\n",
        "                'score': final_score,\n",
        "                'original_score': score,\n",
        "                'method': 'hybrid_reranked'\n",
        "            })\n",
        "        \n",
        "        # Sort in decreasing order by final score    \n",
        "        reranked.sort(key = lambda x: x['score'], reverse = True)  \n",
        "        return reranked\n",
        "\n",
        "    # Helper methods for re-ranking \n",
        "    def _extract_keywords(self, text: str) -> list: \n",
        "        # Extract important keywords from text. \n",
        "        # Simple keyword extraction (when making this more advanced, use NLP libraries but idk)\n",
        "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}\n",
        "        words = text.lower().split()   \n",
        "        keywords = [word for word in words if word not in stop_words and len(word) > 2]\n",
        "        return keywords[:10]  # Limit to top 10 keywords   \n",
        "\n",
        "    def _calculate_keyword_overlap(self, query: str, content: str) -> float:\n",
        "        # Calculate keyword overlap between the query and the content. \n",
        "        query_keywords = set(self._extract_keywords(query))\n",
        "        content_keywords = set(self._extract_keywords(content))\n",
        "        \n",
        "        if not query_keywords: # Size = 0 \n",
        "            return 0.0\n",
        "        \n",
        "        overlap = len(query_keywords.intersection(content_keywords))\n",
        "        return overlap / len(query_keywords) \n",
        "\n",
        "    def _calculate_recency_boost(self, timestamp: str) -> float:\n",
        "        # Calculate recency boost for newer messages.\n",
        "        try:\n",
        "            # Simple recency boost (newer messages get slight preference)\n",
        "            # I don't know if we need time calculations to be accounted for in this function \n",
        "            return 0.10  # Small boost for recency\n",
        "        except:       \n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_length_boost(self, text: str) -> float:\n",
        "        # Calculate boost based on content length. \n",
        "        word_count = len(text.split())  \n",
        "        # Prefer medium-length responses (not too short, not too long)  \n",
        "        if 5 <= word_count <= 50: # Adjust accordingly (I think)  \n",
        "            return 0.2\n",
        "        elif word_count > 50:\n",
        "            return 0.1\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    def _deduplicate_and_select_top(self, results: list, k: int) -> list:\n",
        "        # Remove duplicates and select top k results. \n",
        "        seen_ids = set()\n",
        "        unique_results = []\n",
        "        \n",
        "        for result in results:\n",
        "            if result['id'] not in seen_ids:\n",
        "                seen_ids.add(result['id'])\n",
        "                unique_results.append(result)\n",
        "        \n",
        "        return unique_results[:k]\n",
        "\n",
        "    #Deleting the current model \n",
        "    def reset(self):\n",
        "        #Deletes all vectors from pinecone index.\n",
        "        try:\n",
        "            # Delete all vectors from the index\n",
        "            self.index.delete(delete_all=True)\n",
        "            self.item_counter = 0\n",
        "            print(\"Pinecone index has been reset (all vectors deleted).\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error resetting Pinecone index: {e}. Please try again.\")\n",
        "\n",
        "\n",
        "def start_chat(): \n",
        "    # Global instance of a real Pinecone VDB   \n",
        "    vdb1 = PineconeVectorDB(index) \n",
        "    print(\"Using real Pinecone Vector Database with integrated embeddings\")\n",
        "    vdb1.build() \n",
        "    # Global counter for VDB item IDs \n",
        "    vdb_index_counter = 0 \n",
        "\n",
        "    # No need for get_embedding function - Pinecone handles it automatically\n",
        "    def get_embedding(text: str) -> list:\n",
        "        \"\"\"With integrated models, Pinecone handles embedding generation automatically.\"\"\"\n",
        "        # Return None since we don't need to generate embeddings manually\n",
        "        return None  \n",
        "    \n",
        "    return vdb1, vdb_index_counter, get_embedding\n",
        "\n",
        "# Initialize the Pinecone VDB\n",
        "vdb, vdb_index_counter, get_embedding = start_chat() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaX88JunFB1a"
      },
      "outputs": [],
      "source": [
        "# Simulated Vector Database Class   \n",
        "'''\n",
        "class SimulatedVectorDB:\n",
        "    def __init__(self):\n",
        "        self.items = []  # Stores (item_id, embedding, text_content) tuples\n",
        "        self._is_built = False # Internal flag to simulate index building state\n",
        "        self._embedding_dim = None # To store embedding dimension from first added item\n",
        "\n",
        "    def add_item(self, item_id: int, embedding: list, text_content: str):\n",
        "        \"\"\"Adds an item with its embedding and original text content to the VDB.\"\"\"\n",
        "        if not isinstance(embedding, list):\n",
        "            embedding = embedding.tolist() # Ensure numpy arrays are converted to list\n",
        "        self.items.append((item_id, embedding, text_content))\n",
        "        print(f\"DEBUG: Added item ID {item_id} to VDB (Text: '{text_content[:30]}...')\")\n",
        "\n",
        "\n",
        "    def build(self, n_trees: int):\n",
        "        \"\"\"Simulates building the VDB index. For this simple model, it just sets a flag.\"\"\"\n",
        "        if self._is_built:\n",
        "            raise RuntimeError(\"You can't build a built index. Call reset() first if you want to rebuild.\")\n",
        "\n",
        "        if not self.items:\n",
        "            print(\"WARNING: Building VDB on an empty set of items. Add items first.\")\n",
        "\n",
        "        # In a real VDB, this would optimize the search structure\n",
        "        print(f\"DEBUG: Simulated VDB building with {n_trees} trees. Index is now ready for efficient search.\")\n",
        "        self._is_built = True\n",
        "\n",
        "    def query(self, query_embedding: list, k: int = 1) -> list:\n",
        "        \"\"\"\n",
        "        Simulates querying the VDB. For simplicity, returns the k most recent items\n",
        "        or tries to find a specific ID if the query 'embedding' (here, we'll use a placeholder for actual query)\n",
        "        contains specific instructions (like 'ID: X').\n",
        "\n",
        "        In a real scenario, this would perform a similarity search.\n",
        "        Here, we'll implement a very basic \"retrieval by ID\" or \"latest items\" for demonstration.\n",
        "        \"\"\"\n",
        "        if not self._is_built:\n",
        "            # In a real VDB, query might fail or be inefficient if not built\n",
        "            print(\"WARNING: Querying VDB before it's built. Performance will be poor in a real system.\")\n",
        "\n",
        "        if not self.items: \n",
        "            return []\n",
        "\n",
        "        # Simple simulation: return the latest k items if no specific ID is requested\n",
        "        # For a true RAG, you'd calculate cosine similarity between query_embedding\n",
        "        # and all stored embeddings, then return the top-k most similar.\n",
        "\n",
        "        # To simulate finding by \"ID: X\" in user's example, we'll look for a string in query_embedding\n",
        "        # This is a hack for the 'Retrieved memory based on query (ID: 3)' prompt.\n",
        "        # A real query_embedding would be a list of floats.\n",
        "\n",
        "        # Let's just return the last k items added for now as a general \"retrieval\".\n",
        "        # A true \"query for ID X\" would be handled differently if the user wants specific ID retrieval.\n",
        "        retrieved_results = []\n",
        "        for i in range(1, min(k + 1, len(self.items) + 1)):\n",
        "            item_id, _, text_content = self.items[-i]\n",
        "            retrieved_results.append(f\"Retrieved content (ID: {item_id}): '{text_content}'\")\n",
        "\n",
        "        return retrieved_results\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the VDB, allowing it to be built again.\"\"\"\n",
        "        self.items = []\n",
        "        self._is_built = False\n",
        "        self._embedding_dim = None\n",
        "        print(\"DEBUG: VDB has been reset.\")\n",
        "\n",
        "\n",
        "def start_chat(): \n",
        "    # Global instance of our simulated VDB\n",
        "    vdb = SimulatedVectorDB()\n",
        "    # Global flag to ensure vdb.build() is called only once\n",
        "    vdb_built_flag = False\n",
        "    # Global counter for VDB item IDs\n",
        "    vdb_index_counter = 0\n",
        "\n",
        "    # (Assume get_embedding function is defined elsewhere, e.g., from a model)\n",
        "    # Placeholder for get_embedding if it's not defined in the scope of execution\n",
        "    def get_embedding(text: str) -> list:\n",
        "        \"\"\"Placeholder for an actual embedding generation function.\"\"\"\n",
        "        # In a real scenario, this would call a model to get a vector embedding\n",
        "        # For simulation, just return a dummy embedding based on text length or a hash\n",
        "        return [float(ord(c)) / 100 for c in text[:10]] # A dummy, simple embedding\n",
        "start_chat() \n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxybJxbZFNA9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: VDB has been reset.\n",
            "--- Initializing for test ---\n",
            "\n",
            "--- After adding new messages ---\n",
            "Current chat_history length: 2\n",
            "\n",
            "--- After adding new messages ---\n",
            "Current chat_history length: 4\n",
            "\n",
            "--- Truncation initiated ---\n",
            "Number of messages to truncate: 2\n",
            "DEBUG: Added item ID 0 to VDB (Text: 'Hello, how are you?...')\n",
            "DEBUG: Added item ID 1 to VDB (Text: 'I'm doing well, thank you!...')\n",
            "DEBUG: Simulated VDB building with 10 trees. Index is now ready for efficient search.\n",
            "Truncated 2 messages and stored in VDB.\n",
            "New chat_history length after truncation: 2\n",
            "Total items in VDB: 2\n",
            "\n",
            "--- After adding new messages ---\n",
            "Current chat_history length: 4\n",
            "\n",
            "--- Truncation initiated ---\n",
            "Number of messages to truncate: 2\n",
            "DEBUG: Added item ID 2 to VDB (Text: 'What is the capital of France?...')\n",
            "DEBUG: Added item ID 3 to VDB (Text: 'Paris is the capital of France...')\n",
            "Truncated 2 messages and stored in VDB.\n",
            "New chat_history length after truncation: 2\n",
            "Total items in VDB: 4\n",
            "\n",
            "--- Retrieval from VDB ---\n",
            "Retrieved content (ID: 3): 'Paris is the capital of France.'\n",
            "Retrieved content (ID: 2): 'What is the capital of France?'\n"
          ]
        }
      ],
      "source": [
        "# @title Chat History Management and Truncation\n",
        "# Initialize global chat_history (if not already done)\n",
        "chat_history = [] \n",
        "\n",
        "\n",
        "def manage_chat_history(user_message, system_response):\n",
        "    global chat_history, vdb_index_counter, vdb_built_flag, vdb # Ensure vdb is global here\n",
        "\n",
        "    chat_history.append({\"role\": \"user\", \"parts\": [user_message]})\n",
        "    chat_history.append({\"role\": \"model\", \"parts\": [system_response]})\n",
        "\n",
        "    print(f\"\\n--- After adding new messages ---\")\n",
        "    print(f\"Current chat_history length: {len(chat_history)}\")\n",
        "\n",
        "    if len(chat_history) > MAX_CHAT_HISTORY_LENGTH * 2:\n",
        "        num_to_remove = len(chat_history) - MAX_CHAT_HISTORY_LENGTH * 2\n",
        "        messages_to_store = chat_history[:num_to_remove]\n",
        "        chat_history = chat_history[num_to_remove:]\n",
        "\n",
        "        print(f\"\\n--- Truncation initiated ---\")\n",
        "        print(f\"Number of messages to truncate: {num_to_remove}\")\n",
        "\n",
        "        # Add items to VDB\n",
        "        for message in messages_to_store:\n",
        "            text_content = message[\"parts\"][0]\n",
        "            embedding = get_embedding(text_content) # Assuming get_embedding is available\n",
        "            vdb.add_item(vdb_index_counter, embedding, text_content) # Pass text_content\n",
        "            vdb_index_counter += 1\n",
        "\n",
        "        # Only build the index ONCE, after the first batch of items is added\n",
        "        if not vdb_built_flag and vdb_index_counter > 0:\n",
        "            vdb.build(10) # Build the index with 10 trees (parameter is simulated)\n",
        "            vdb_built_flag = True # Set flag to True so it's not built again\n",
        "\n",
        "        print(f\"Truncated {num_to_remove} messages and stored in VDB.\")\n",
        "        print(f\"New chat_history length after truncation: {len(chat_history)}\")\n",
        "        print(f\"Total items in VDB: {vdb_index_counter}\")\n",
        "\n",
        "# --- Test the functionality ---\n",
        "# Reset VDB for a clean test run\n",
        "vdb.reset()\n",
        "vdb_built_flag = False\n",
        "vdb_index_counter = 0\n",
        "chat_history = [] # Reset chat history too for a clean start\n",
        "\n",
        "print(\"--- Initializing for test ---\")\n",
        "\n",
        "# Simulate some conversation\n",
        "manage_chat_history(\"Hello, how are you?\", \"I'm doing well, thank you!\")\n",
        "manage_chat_history(\"What is the capital of France?\", \"Paris is the capital of France.\")\n",
        "\n",
        "# This should trigger truncation and VDB storage\n",
        "manage_chat_history(\"Can you tell me more about AI?\", \"AI is a rapidly evolving field.\")\n",
        "\n",
        "# Simulate retrieval from VDB\n",
        "print(f\"\\n--- Retrieval from VDB ---\")\n",
        "# In a real RAG, you'd query with an embedding of the current user input.\n",
        "# Here, we'll just demonstrate retrieving the latest few items from the VDB for completeness.\n",
        "query_results = vdb.query(get_embedding(\"dummy query for retrieval\"), k=2) # k=2 to get more than one\n",
        "if query_results:\n",
        "    for result in query_results:\n",
        "        print(result)\n",
        "else:\n",
        "    print(\"VDB is empty. No retrieval.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar2VBY-tFmer"
      },
      "outputs": [],
      "source": [
        "# @title Main Chat Functionality\n",
        "'''\n",
        "Given to me \n",
        "\n",
        "\n",
        "# Define retrieve_from_vdb function\n",
        "def retrieve_from_vdb(query_text: str) -> list:\n",
        "    \"\"\"\n",
        "    Retrieves relevant context from the VDB based on the query text.\n",
        "    Returns a list of strings, where each string is the content of a retrieved memory.\n",
        "    \"\"\"\n",
        "    global vdb # Ensure vdb is accessible\n",
        "\n",
        "    if not vdb._is_built and vdb.items:\n",
        "        print(\"WARNING: Querying VDB before it's built. Results may not be optimal.\")\n",
        "\n",
        "    if not vdb.items:\n",
        "        print(\"--- VDB is empty. No retrieval. ---\")\n",
        "        return []\n",
        "\n",
        "    query_embedding = get_embedding(query_text)\n",
        "    # The vdb.query method now returns formatted strings, so we need to parse them.\n",
        "    raw_retrieved_items = vdb.query(query_embedding, k=2) # Retrieve top 2 items\n",
        "    \n",
        "    extracted_contexts = []\n",
        "    if raw_retrieved_items:\n",
        "        print(\"--- Retrieval from VDB ---\")\n",
        "        print(f\"Query for retrieval: '{query_text}'\")\n",
        "        for item_str in raw_retrieved_items:\n",
        "            # Example format from vdb.query: \"Retrieved content (ID: 1): 'Hello, how are you?'\"\n",
        "            # We want to extract just 'Hello, how are you?'\n",
        "            parts = item_str.split(\"': '\")\n",
        "            if len(parts) > 1:\n",
        "                text_content = parts[1].rstrip(\"'\") # Get the text part and remove trailing single quote\n",
        "                extracted_contexts.append(text_content)\n",
        "            else:\n",
        "                extracted_contexts.append(item_str) # Fallback if format is unexpected\n",
        "\n",
        "    # Print the actual content retrieved\n",
        "    if extracted_contexts:\n",
        "        print(\"Retrieved content:\")\n",
        "        for context in extracted_contexts:\n",
        "            print(f\"- '{context}'\")\n",
        "\n",
        "    return extracted_contexts\n",
        "'''\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "def retrieve_from_vdb(query_text: str) -> list:       \n",
        "    # Better retrieval with multiple strategies and intelligent context selection.\n",
        "    # Returns a list of strings with the most relevant past conversations.\n",
        "    \n",
        "    global vdb\n",
        "\n",
        "    if not vdb._is_built:   \n",
        "        print(\"Pinecone index is always ready for querying: No worries\")   \n",
        "\n",
        "    # Contextual query enhancement\n",
        "    enhanced_query = _enhance_query_with_context(query_text, chat_history)\n",
        "    \n",
        "    # Use advanced semantic search\n",
        "    raw_retrieved_items = vdb.query(enhanced_query, k=3)  # get more results for better selection \n",
        "    \n",
        "    extracted_contexts = []\n",
        "    if raw_retrieved_items:\n",
        "        print(\"Advanced Retrieval from Pinecone VDB\")\n",
        "        print(f\"Original query: '{query_text}'\")\n",
        "        print(f\"Enhanced query: '{enhanced_query}'\")\n",
        "        \n",
        "        for item_str in raw_retrieved_items:   \n",
        "            # Parse results with method information\n",
        "            parts = item_str.split(\"': '\")\n",
        "            if len(parts) > 1:\n",
        "                text_content = parts[1].rstrip(\"'\")\n",
        "                extracted_contexts.append(text_content)\n",
        "            else:\n",
        "                extracted_contexts.append(item_str)\n",
        "\n",
        "    # Intelligent context selection\n",
        "    if extracted_contexts: \n",
        "        print(\"Retrieved content (with better ranking):\")\n",
        "        for i, context in enumerate(extracted_contexts, 1):\n",
        "            print(f\"{i}. '{context}'\")\n",
        "        \n",
        "        # Select most relevant contexts based on current conversation\n",
        "        selected_contexts = _select_most_relevant_contexts(extracted_contexts, query_text, chat_history)\n",
        "        return selected_contexts\n",
        "\n",
        "    return []    \n",
        "\n",
        "# Query enhancement with conversation context\n",
        "def _enhance_query_with_context(query_text: str, chat_history: list) -> str:\n",
        "    # Enhance query using recent conversation context. \n",
        "    if not chat_history:\n",
        "        return query_text\n",
        "    \n",
        "    # Extract recent conversation topics\n",
        "    recent_messages = []\n",
        "    for entry in chat_history[-4:]:  # Last 4 messages\n",
        "        recent_messages.append(entry['parts'][0])\n",
        "    \n",
        "    # Add context keywords to query\n",
        "    context_keywords = _extract_context_keywords(recent_messages)\n",
        "    if context_keywords:\n",
        "        enhanced_query = f\"{query_text} {context_keywords}\"\n",
        "        return enhanced_query\n",
        "    \n",
        "    return query_text\n",
        "\n",
        "# Context keyword extraction\n",
        "def _extract_context_keywords(messages: list) -> str:\n",
        "    # Extract important keywords from recent conversation.    \n",
        "    all_text = \" \".join(messages)\n",
        "    words = all_text.lower().split()\n",
        "    \n",
        "    # Simple keyword frequency analysis\n",
        "    word_freq = {}\n",
        "    for word in words:\n",
        "        if len(word) > 3 and word.isalpha():\n",
        "            word_freq[word] = word_freq.get(word, 0) + 1\n",
        "    \n",
        "    # Get most frequent words\n",
        "    keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "    return \" \".join([word for word, freq in keywords])\n",
        "\n",
        "# Intelligent context selection\n",
        "def _select_most_relevant_contexts(contexts: list, query_text: str, chat_history: list) -> list:\n",
        "    #Select the most relevant contexts based on current conversation. \n",
        "    if len(contexts) <= 2: \n",
        "        return contexts\n",
        "    \n",
        "    # Score contexts based on relevance to current conversation\n",
        "    scored_contexts = []\n",
        "    for context in contexts:\n",
        "        score = _calculate_context_relevance(context, query_text, chat_history)\n",
        "        scored_contexts.append((context, score))\n",
        "    \n",
        "    # Sort by relevance score and return top 2\n",
        "    scored_contexts.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [context for context, score in scored_contexts[:2]]\n",
        "\n",
        "# Context relevance calculation\n",
        "def _calculate_context_relevance(context: str, query_text: str, chat_history: list) -> float:\n",
        "    # Calculate how relevant a context is to the current conversation.  \n",
        "    score = 0.0\n",
        "    \n",
        "    # Query similarity\n",
        "    query_words = set(query_text.lower().split())\n",
        "    context_words = set(context.lower().split())\n",
        "    query_overlap = len(query_words.intersection(context_words)) / max(len(query_words), 1)\n",
        "    score += query_overlap * 0.5\n",
        "    \n",
        "    # Recent conversation similarity\n",
        "    if chat_history:\n",
        "        recent_text = \" \".join([entry['parts'][0] for entry in chat_history[-2:]])\n",
        "        recent_words = set(recent_text.lower().split())\n",
        "        recent_overlap = len(recent_words.intersection(context_words)) / max(len(recent_words), 1)\n",
        "        score += recent_overlap * 0.3\n",
        "    \n",
        "    # Content quality (prefer longer, more informative responses)\n",
        "    if len(context.split()) > 10: #idk if it should be > 10 \n",
        "        score += 0.2\n",
        "    \n",
        "    return score\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "def chat_with_gemini_with_memory():\n",
        "    print(\"Welcome to the Pseudo-infinite Chatbot! Type 'exit' to end the conversation.\")\n",
        "    # Assuming 'model' is defined and initialized elsewhere (e.g., gemini-pro)\n",
        "    # Assuming 'chat_history' is globally initialized as an empty list\n",
        "\n",
        "    while True:\n",
        "        user_message = input(\"You: \")\n",
        "        if user_message.lower() == 'exit':\n",
        "            print(\"Chat ended.\")\n",
        "            break\n",
        "\n",
        "        # Retrieve relevant context from VDB\n",
        "        # This function will now correctly return a list of text strings\n",
        "        retrieved_context = retrieve_from_vdb(user_message)\n",
        "        context_prompt = \"\"\n",
        "        if retrieved_context:\n",
        "            context_prompt = \"The user has previously discussed the following:\\n\" + \"\\n\".join(retrieved_context) + \"\\n\"\n",
        "\n",
        "        # Construct the full prompt for the LLM\n",
        "        # Prepend the retrieved context to the system prompt\n",
        "        full_prompt = f\"{context_prompt}Current conversation:\\n\"\n",
        "        for entry in chat_history:\n",
        "            role = \"User\" if entry[\"role\"] == \"user\" else \"Model\"\n",
        "            full_prompt += f\"{role}: {entry['parts'][0]}\\n\"\n",
        "        full_prompt += f\"User: {user_message}\\nModel:\"\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Generate Gemini's response\n",
        "            # Ensure 'model' is properly initialized (e.g., genai.GenerativeModel('gemini-pro'))\n",
        "            response = model.generate_content(\n",
        "                contents=[{\"role\": \"user\", \"parts\": [full_prompt]}]\n",
        "            )\n",
        "            gemini_response = response.candidates[0].content.parts[0].text\n",
        "            print(f\"Gemini: {gemini_response}\")\n",
        "\n",
        "            # Manage chat history (truncate and store in VDB if needed)\n",
        "            manage_chat_history(user_message, gemini_response)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            print(\"Please check your API key and ensure the model is accessible.\")\n",
        "\n",
        "# Note: Before running chat_with_gemini_with_memory(), ensure you have:\n",
        "# - Initialized your Gemini 'model' object (e.g., import google.generativeai as genai; model = genai.GenerativeModel('gemini-pro'))\n",
        "# - Set your Google API Key (genai.configure(api_key=\"YOUR_API_KEY\"))\n",
        "# - Run the SimulatedVectorDB class definition and the initial global variable setup (vdb, vdb_built_flag, vdb_index_counter, chat_history)\n",
        "# - Run the manage_chat_history function definition.\n",
        "\n",
        "# Example of how you would set up the globals and start the chat:\n",
        "# import google.generativeai as genai\n",
        "# import os\n",
        "# genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\")) # Or your actual key\n",
        "# model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "'''\n",
        "chat_history = []\n",
        "vdb = SimulatedVectorDB() # Re-initialize vdb if needed for a fresh chat session\n",
        "vdb_built_flag = False\n",
        "vdb_index_counter = 0 \n",
        "'''\n",
        "\n",
        "chat_with_gemini_with_memory()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
