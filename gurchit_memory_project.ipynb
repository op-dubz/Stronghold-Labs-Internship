{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhcX37ZAA03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " :( thats not good. maybe use the basic vdb instead\n",
            "Successfully connected to Pinecone index with integrated embeddings\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n#define Pinecone index name and dimension\\nPINECONE_INDEX_NAME = \"chatbot-memory\"\\nEMBEDDING_DIMENSION = 768  # standard dimension for many embedding models\\n\\n#create Pinecone index if it doesn\\'t exist\\ntry:\\n    # check if index already exists\\n    if PINECONE_INDEX_NAME not in pinecone.list_indexes():\\n        # create new index with proper schema\\n        pinecone.create_index(\\n            name=PINECONE_INDEX_NAME,\\n            dimension=EMBEDDING_DIMENSION,\\n            metric=\"cosine\",  # use cosine similarity for text embeddings\\n            spec=pinecone.Spec(\\n                serverless=pinecone.ServerlessSpec(\\n                    cloud=\"aws\",\\n                    region=\"us-east-1\"\\n                )\\n            )\\n        )\\n        print(f\"Created new Pinecone index \\'{PINECONE_INDEX_NAME}\\'\")\\n    else:\\n        print(f\"Using existing Pinecone index \\'{PINECONE_INDEX_NAME}\\'\")\\n\\n    # connect to the index\\n    index = pinecone.Index(PINECONE_INDEX_NAME)\\n    print(f\"Successfully connected to Pinecone index\")\\n\\nexcept Exception as e:\\n    print(f\"Error initializing Pinecone: {e}\")\\n    # fallback to simulated VDB if Pinecone fails\\n    index = None  \\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title Setup and Imports\n",
        "\n",
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError \n",
        "import json \n",
        "\n",
        "import time\n",
        "import statistics\n",
        "from typing import Dict, List, Tuple, Any \n",
        "from dataclasses import dataclass, asdict \n",
        "from datetime import datetime, timedelta \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "\n",
        "\n",
        "\n",
        "def get_secret(sec):\n",
        "    secret_name = sec\n",
        "    region_name = \"us-east-2\"\n",
        "    # Create a Secrets Manager client\n",
        "    session = boto3.session.Session()\n",
        "    client = session.client(\n",
        "        service_name='secretsmanager',\n",
        "        region_name=region_name\n",
        "    )\n",
        "    # AKIAV62IILI7ED3DDIHR   7UpChxKz74Y7DbhaqSUotZzWFvbJPP6tXdTpyZvI\n",
        "    try:\n",
        "        get_secret_value_response = client.get_secret_value(\n",
        "            SecretId=secret_name\n",
        "        )\n",
        "    except ClientError as e:\n",
        "        # For a list of exceptions thrown, see\n",
        "        # https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
        "        raise e\n",
        "    secret = json.loads(get_secret_value_response['SecretString'])\n",
        "    return secret['pinecone'], secret['gemini'] \n",
        "\n",
        "pineconeAPIKEY, geminiAPIKEY = get_secret(\"strongholdLabs\") \n",
        "genai.configure(api_key=geminiAPIKEY) \n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "# Chat history and truncation settings\n",
        "chat_history = []\n",
        "MAX_CHAT_HISTORY_LENGTH = 4 # Number of recent turns to keep in active memory, so # of messages by user + chatbot = 2 * chat_history_length (I think) \n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "#initialize pinecone client\n",
        "pc = Pinecone(api_key=pineconeAPIKEY)  \n",
        "PINECONE_INDEX_NAME = \"chatbot-memory-integrated\" \n",
        "if not pc.has_index(PINECONE_INDEX_NAME):\n",
        "    pc.create_index_for_model(\n",
        "        name = PINECONE_INDEX_NAME,      \n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\",\n",
        "        embed = { \n",
        "            \"model\": \"llama-text-embed-v2\", #Does this allow for auto embedding without needing an embedding function?? I think so.  \n",
        "            \"field_map\": {\"text\": \"message_text\"}\n",
        "        }\n",
        "    )\n",
        "    print(f\"Created new Pinecone index '{PINECONE_INDEX_NAME}' with integrated embedding model\")\n",
        "else:\n",
        "    print(f\" :( thats not good. maybe use the basic vdb instead\")  \n",
        "\n",
        "index = pc.Index(PINECONE_INDEX_NAME)\n",
        "print(f\"Successfully connected to Pinecone index with integrated embeddings\")\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#i dont think we need this part below. \n",
        "\n",
        "'''\n",
        "#define Pinecone index name and dimension\n",
        "PINECONE_INDEX_NAME = \"chatbot-memory\"\n",
        "EMBEDDING_DIMENSION = 768  # standard dimension for many embedding models\n",
        "\n",
        "#create Pinecone index if it doesn't exist\n",
        "try:\n",
        "    # check if index already exists\n",
        "    if PINECONE_INDEX_NAME not in pinecone.list_indexes():\n",
        "        # create new index with proper schema\n",
        "        pinecone.create_index(\n",
        "            name=PINECONE_INDEX_NAME,\n",
        "            dimension=EMBEDDING_DIMENSION,\n",
        "            metric=\"cosine\",  # use cosine similarity for text embeddings\n",
        "            spec=pinecone.Spec(\n",
        "                serverless=pinecone.ServerlessSpec(\n",
        "                    cloud=\"aws\",\n",
        "                    region=\"us-east-1\"\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        print(f\"Created new Pinecone index '{PINECONE_INDEX_NAME}'\")\n",
        "    else:\n",
        "        print(f\"Using existing Pinecone index '{PINECONE_INDEX_NAME}'\")\n",
        "    \n",
        "    # connect to the index\n",
        "    index = pinecone.Index(PINECONE_INDEX_NAME)\n",
        "    print(f\"Successfully connected to Pinecone index\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Pinecone: {e}\")\n",
        "    # fallback to simulated VDB if Pinecone fails\n",
        "    index = None  \n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using real Pinecone Vector Database with integrated embeddings\n",
            "Pinecone index is already built and optimized for similarity search.\n"
          ]
        }
      ],
      "source": [
        "# Pinecone Vector Database Class (replacing SimulatedVectorDB) \n",
        "class PineconeVectorDB:\n",
        "    def __init__(self, index): \n",
        "        self.index = index  # Real Pinecone index connection\n",
        "        self.item_counter = 0  # Track item IDs\n",
        "\n",
        "    def add_item(self, item_id: int, embedding: list, text_content: str):\n",
        "        \"\"\"Adds an item with text content - Pinecone handles embedding generation.\"\"\"\n",
        "        # with integrated models, we send text directly: no need for embeddings\n",
        "        # Enhanced metadata for better retrieval \n",
        "        metadata = {\n",
        "            \"item_id\": str(item_id),\n",
        "            \"timestamp\": str(np.datetime64('now')), \n",
        "            \"message_type\": \"chat_message\",\n",
        "            \"text_content\": text_content,\n",
        "            \"word_count\": len(text_content.split()),  # For chunking optimization \n",
        "            \"keywords\": self._extract_keywords(text_content)  # For hybrid search  \n",
        "        }  \n",
        "        \n",
        "        # Upsert with text: Pinecone generates embeddings automatically\n",
        "        self.index.upsert(\n",
        "            vectors=[(str(item_id), {\"message_text\": text_content}, metadata)]\n",
        "        )   \n",
        "        \n",
        "        print(f\"Added item ID {item_id} to Pinecone VDB (Text: '{text_content[:30]}...')\")\n",
        "    \n",
        "    def _detect_topic(self, conversation_text: str) -> str:\n",
        "        # Use Gemini to detect the main topic of a conversation turn. \n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "            Analyze this conversation text and identify the main topic in 2-3 words:\n",
        "            \"{conversation_text}\"\n",
        "            \n",
        "            Return only the topic, no explanation. Examples: \"AI programming\", \"Python debugging\", \"API integration\"\n",
        "            \"\"\"\n",
        "            \n",
        "            response = self.model.generate_content(prompt)\n",
        "            topic = response.text.strip()\n",
        "            return topic if topic else \"general\"\n",
        "        except Exception as e:\n",
        "            print(f\"Topic detection failed: {e}\")\n",
        "            return \"general\"\n",
        "    \n",
        "    def _expand_query_semantically(self, query_text: str, chat_history: list) -> list:\n",
        "        #Use Gemini to expand queries semantically based on intent and context.\n",
        "        try:\n",
        "            # Get recent conversation context\n",
        "            recent_context = \"\"\n",
        "            if chat_history:\n",
        "                recent_messages = [entry['parts'][0] for entry in chat_history[-4:]]\n",
        "                recent_context = \" \".join(recent_messages)\n",
        "            \n",
        "            prompt = f\"\"\"\n",
        "            Given this user query: \"{query_text}\"\n",
        "            And recent conversation context: \"{recent_context}\"\n",
        "            \n",
        "            Generate 2-3 semantically equivalent ways to express the same intent.\n",
        "            Focus on different phrasings that capture the same meaning.\n",
        "            \n",
        "            Return each expansion on a new line, no numbering or explanation.\n",
        "            \"\"\"\n",
        "            \n",
        "            response = self.model.generate_content(prompt)\n",
        "            expansions = [line.strip() for line in response.text.split('\\n') if line.strip()]\n",
        "            \n",
        "            # Always include original query\n",
        "            all_queries = [query_text] + expansions\n",
        "            return all_queries[:4]  \n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Semantic query expansion failed: {e}\")\n",
        "            return [query_text]\n",
        "    \n",
        "    \n",
        "    # IM NOT SURE IF THIS IS GOOD  \n",
        "    def _score_semantic_relevance(self, query: str, chunk_text: str) -> float:\n",
        "        # Use Gemini to score semantic relevance between query and chunk. \n",
        "        try:  \n",
        "            prompt = f\"\"\"\n",
        "            Rate the semantic relevance between this query and content on a scale of 0.0 to 1.0.\n",
        "            \n",
        "            Query: \"{query}\"\n",
        "            Content: \"{chunk_text}\"\n",
        "            \n",
        "            Consider:\n",
        "            - Does the content directly answer the query?\n",
        "            - Is the content contextually relevant?\n",
        "            - Does it provide useful information for the query?\n",
        "            \n",
        "            Return only the score (0.0 to 1.0), no explanation.\n",
        "            \"\"\"\n",
        "            response = self.model.generate_content(prompt)\n",
        "            try:\n",
        "                score = float(response.text.strip())\n",
        "                return max(0.0, min(1.0, score))  # Clamp between 0.0 and 1.0\n",
        "            except ValueError:\n",
        "                return 0.5  # Default score if parsing fails\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Semantic scoring failed: {e}\")\n",
        "            return 0.5\n",
        "    \n",
        "    def _create_conversation_chunk(self, user_message: str, assistant_response: str, turn_number: int) -> dict:\n",
        "        # Create a semantically coherent conversation chunk. \n",
        "        # Combine user and assistant messages\n",
        "        full_conversation = f\"User: {user_message}\\nAssistant: {assistant_response}\"\n",
        "        # Detect topic using Gemini\n",
        "        topic = self._detect_topic(full_conversation)\n",
        "        \n",
        "        # Create enhanced metadata\n",
        "        metadata = {\n",
        "            \"conversation_turn\": turn_number,\n",
        "            \"topic\": topic,\n",
        "            \"user_message\": user_message,\n",
        "            \"assistant_response\": assistant_response,\n",
        "            \"full_conversation\": full_conversation,\n",
        "            \"message_type\": \"conversation_chunk\",\n",
        "            \"timestamp\": str(np.datetime64('now')),\n",
        "            \"word_count\": len(full_conversation.split()),\n",
        "            \"chunk_id\": f\"turn_{turn_number}_{topic.lower().replace(' ', '_')}\"\n",
        "        }\n",
        "        \n",
        "        return full_conversation, metadata\n",
        "    \n",
        "    def query(self, query_text: str, k: int = 1) -> list:  \n",
        "        # Advanced semantic search using Gemini for genuine understanding. \n",
        "        try:\n",
        "            # Semantic query expansion\n",
        "            expanded_queries = self._expand_query_semantically(query_text, chat_history or [])\n",
        "            \n",
        "            all_results = [] \n",
        "            \n",
        "            for expanded_query in expanded_queries:\n",
        "                # Get initial results from Pinecone\n",
        "                query_results = self.index.query(\n",
        "                    vector={\"message_text\": expanded_query},\n",
        "                    top_k = k * 3,  # Get more candidates for semantic filtering\n",
        "                    include_metadata=True\n",
        "                )\n",
        "                \n",
        "                # Semantic relevance scoring using Gemini\n",
        "                scored_results = []\n",
        "                for match in query_results.matches:\n",
        "                    chunk_text = match.metadata.get('full_conversation', '')\n",
        "                    if chunk_text:\n",
        "                        # Use Gemini to score semantic relevance\n",
        "                        semantic_score = self._score_semantic_relevance(expanded_query, chunk_text)\n",
        "                        \n",
        "                        # Apply a threshold (0.75 for business use I think) \n",
        "                        if semantic_score >= 0.75:\n",
        "                            scored_results.append({\n",
        "                                'id': match.id,\n",
        "                                'text_content': chunk_text,\n",
        "                                'semantic_score': semantic_score,\n",
        "                                'pinecone_score': match.score,\n",
        "                                'topic': match.metadata.get('topic', 'unknown'),\n",
        "                                'turn_number': match.metadata.get('conversation_turn', 0),\n",
        "                                'method': 'semantic_search'\n",
        "                            })\n",
        "                \n",
        "                all_results.extend(scored_results)\n",
        "            \n",
        "            # Remove duplicates and sort by semantic relevance\n",
        "            unique_results = self._deduplicate_and_select_top(all_results, k * 2)\n",
        "            \n",
        "            # Final ranking: semantic score (70%) + topic relevance (20%) + recency (10%)\n",
        "            for result in unique_results:\n",
        "                topic_boost = 0.2 if result['topic'] in query_text.lower() else 0.0\n",
        "                recency_boost = 0.1 * (1.0 / (result['turn_number'] + 1))  # Newer turns get slight boost\n",
        "                \n",
        "                result['final_score'] = (\n",
        "                    result['semantic_score'] * 0.7 + \n",
        "                    topic_boost + \n",
        "                    recency_boost\n",
        "                )\n",
        "            \n",
        "            # Sort by final score and return top k\n",
        "            unique_results.sort(key=lambda x: x['final_score'], reverse=True)\n",
        "            final_results = unique_results[:k]\n",
        "            \n",
        "            # Format results for return\n",
        "            retrieved_results = []\n",
        "            for result in final_results:\n",
        "                retrieved_results.append({\n",
        "                    'id': result['id'],\n",
        "                    'content': result['text_content'],\n",
        "                    'relevance_score': result['final_score'],\n",
        "                    'topic': result['topic'],\n",
        "                    'conversation_turn': result['turn_number'],\n",
        "                    'method': result['method']\n",
        "                })\n",
        "            \n",
        "            return retrieved_results\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error in semantic search: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def add_conversation_turn(self, user_message: str, assistant_response: str, turn_number: int):\n",
        "        # Add a complete conversation turn as a semantic chunk. \n",
        "        try:\n",
        "            # Create conversation chunk with enhanced metadata\n",
        "            chunk_text, metadata = self._create_conversation_chunk(user_message, assistant_response, turn_number)\n",
        "            \n",
        "            # Add to Pinecone because it already handles embedding generation \n",
        "            self.index.upsert(\n",
        "                vectors=[(metadata['chunk_id'], {\"message_text\": chunk_text}, metadata)]\n",
        "            )\n",
        "            \n",
        "            print(f\"Added conversation turn {turn_number} (Topic: {metadata['topic']}) to VDB\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error adding conversation turn: {e}\")\n",
        "    \n",
        "    '''\n",
        "    # Re-ranking Method. NEED HELP CUZ IDK IF ITS GOOD \n",
        "    def _rerank_results(self, matches, original_query: str) -> list:\n",
        "        #Re-rank results based on multiple relevance factors. \n",
        "        \n",
        "        reranked = []\n",
        "        \n",
        "        for match in matches:\n",
        "            score = match.score\n",
        "            text_content = match.metadata.get('text_content', '')\n",
        "            \n",
        "            # Boost score based on keyword overlap\n",
        "            keyword_boost = self._calculate_keyword_overlap(original_query, text_content)\n",
        "            \n",
        "            # Boost score based on recency (newer messages slightly preferred)\n",
        "            recency_boost = self._calculate_recency_boost(match.metadata.get('timestamp', ''))\n",
        "            \n",
        "            # Boost score based on content length (prefer meaningful responses)\n",
        "            length_boost = self._calculate_length_boost(text_content)\n",
        "            \n",
        "            # Combined score with weights\n",
        "            final_score = (score * 0.6 + keyword_boost * 0.2 + recency_boost * 0.1 + length_boost * 0.1)\n",
        "            \n",
        "            reranked.append({\n",
        "                'id': match.id,\n",
        "                'text_content': text_content,\n",
        "                'score': final_score,\n",
        "                'original_score': score,\n",
        "                'method': 'hybrid_reranked'\n",
        "            })\n",
        "        \n",
        "        # Sort in decreasing order by final score    \n",
        "        reranked.sort(key = lambda x: x['score'], reverse = True)  \n",
        "        return reranked\n",
        "\n",
        "    # Helper methods for re-ranking \n",
        "    def _extract_keywords(self, text: str) -> list: \n",
        "        # Extract important keywords from text. \n",
        "        # Simple keyword extraction (when making this more advanced, use NLP libraries but idk)\n",
        "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}\n",
        "        words = text.lower().split()   \n",
        "        keywords = [word for word in words if word not in stop_words and len(word) > 2]\n",
        "        return keywords[:10]  # Limit to top 10 keywords   \n",
        "\n",
        "    def _calculate_keyword_overlap(self, query: str, content: str) -> float:\n",
        "        # Calculate keyword overlap between the query and the content. \n",
        "        query_keywords = set(self._extract_keywords(query))\n",
        "        content_keywords = set(self._extract_keywords(content))\n",
        "        \n",
        "        if not query_keywords: # Size = 0 \n",
        "            return 0.0\n",
        "        \n",
        "        overlap = len(query_keywords.intersection(content_keywords))\n",
        "        return overlap / len(query_keywords) \n",
        "    ''' \n",
        "\n",
        "    def _deduplicate_and_select_top(self, results: list, k: int) -> list:\n",
        "        # Remove duplicates and select top k results. \n",
        "        seen_ids = set()\n",
        "        unique_results = []\n",
        "        \n",
        "        for result in results:\n",
        "            if result['id'] not in seen_ids:\n",
        "                seen_ids.add(result['id'])\n",
        "                unique_results.append(result)\n",
        "        \n",
        "        return unique_results[:k]\n",
        "\n",
        "    #Deleting the current model \n",
        "    def reset(self):\n",
        "        #Deletes all vectors from pinecone index.\n",
        "        try:\n",
        "            # Delete all vectors from the index\n",
        "            self.index.delete(delete_all=True)\n",
        "            self.item_counter = 0\n",
        "            print(\"Pinecone index has been reset (all vectors deleted).\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error resetting Pinecone index: {e}. Please try again.\")\n",
        "\n",
        "\n",
        "def start_chat(): \n",
        "    # Global instance of a real Pinecone VDB   \n",
        "    vdb1 = PineconeVectorDB(index) \n",
        "    print(\"Using real Pinecone Vector Database with integrated embeddings\")\n",
        "    # Global counter for VDB item IDs \n",
        "    vdb_index_counter = 0 \n",
        "    return vdb1, vdb_index_counter \n",
        "\n",
        "# Initialize the Pinecone VDB\n",
        "vdb, vdb_index_counter = start_chat()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxybJxbZFNA9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: VDB has been reset.\n",
            "--- Initializing for test ---\n",
            "\n",
            "--- After adding new messages ---\n",
            "Current chat_history length: 2\n",
            "\n",
            "--- After adding new messages ---\n",
            "Current chat_history length: 4\n",
            "\n",
            "--- Truncation initiated ---\n",
            "Number of messages to truncate: 2\n",
            "DEBUG: Added item ID 0 to VDB (Text: 'Hello, how are you?...')\n",
            "DEBUG: Added item ID 1 to VDB (Text: 'I'm doing well, thank you!...')\n",
            "DEBUG: Simulated VDB building with 10 trees. Index is now ready for efficient search.\n",
            "Truncated 2 messages and stored in VDB.\n",
            "New chat_history length after truncation: 2\n",
            "Total items in VDB: 2\n",
            "\n",
            "--- After adding new messages ---\n",
            "Current chat_history length: 4\n",
            "\n",
            "--- Truncation initiated ---\n",
            "Number of messages to truncate: 2\n",
            "DEBUG: Added item ID 2 to VDB (Text: 'What is the capital of France?...')\n",
            "DEBUG: Added item ID 3 to VDB (Text: 'Paris is the capital of France...')\n",
            "Truncated 2 messages and stored in VDB.\n",
            "New chat_history length after truncation: 2\n",
            "Total items in VDB: 4\n",
            "\n",
            "--- Retrieval from VDB ---\n",
            "Retrieved content (ID: 3): 'Paris is the capital of France.'\n",
            "Retrieved content (ID: 2): 'What is the capital of France?'\n"
          ]
        }
      ],
      "source": [
        "# Chat History Management and Truncation\n",
        "\n",
        "def manage_chat_history(user_message, system_response):\n",
        "    global chat_history, vdb_index_counter, vdb\n",
        "    \n",
        "    chat_history.append({\"role\": \"user\", \"parts\": [user_message]})\n",
        "    chat_history.append({\"role\": \"model\", \"parts\": [system_response]})\n",
        "    \n",
        "    print(f\"\\n--- After adding new messages ---\")\n",
        "    print(f\"Current chat_history length: {len(chat_history)}\")\n",
        "    \n",
        "    if len(chat_history) > MAX_CHAT_HISTORY_LENGTH * 2:\n",
        "        num_to_remove = len(chat_history) - MAX_CHAT_HISTORY_LENGTH * 2\n",
        "        messages_to_store = chat_history[:num_to_remove]\n",
        "        chat_history = chat_history[num_to_remove:]\n",
        "        \n",
        "        print(f\"\\n--- Truncation initiated ---\")\n",
        "        print(f\"Number of messages to truncate: {num_to_remove}\")\n",
        "        \n",
        "        # Store conversation turns (user + assistant pairs)\n",
        "        turn_number = 1\n",
        "        for i in range(0, len(messages_to_store), 2):\n",
        "            if i + 1 < len(messages_to_store):\n",
        "                user_msg = messages_to_store[i][\"parts\"][0]\n",
        "                assistant_msg = messages_to_store[i + 1][\"parts\"][0]\n",
        "                \n",
        "                # Use new conversation chunk method\n",
        "                vdb.add_conversation_turn(user_msg, assistant_msg, turn_number)\n",
        "                turn_number += 1\n",
        "        \n",
        "        print(f\"Truncated {num_to_remove} messages and stored {turn_number-1} conversation turns in VDB.\")\n",
        "        print(f\"New chat_history length after truncation: {len(chat_history)}\")\n",
        "    \n",
        "    return chat_history\n",
        "\n",
        "# --- Test the functionality ---\n",
        "'''\n",
        "print(\"--- Initializing for test ---\")\n",
        "\n",
        "# Simulate some conversation\n",
        "manage_chat_history(\"Hello, how are you?\", \"I'm doing well, thank you!\")\n",
        "manage_chat_history(\"What is the capital of France?\", \"Paris is the capital of France.\")\n",
        "\n",
        "# This should trigger truncation and VDB storage\n",
        "manage_chat_history(\"Can you tell me more about AI?\", \"AI is a rapidly evolving field.\")\n",
        "\n",
        "# Simulate retrieval from VDB\n",
        "print(f\"\\n--- Retrieval from VDB ---\")\n",
        "\n",
        "# In a real RAG, you'd query with an embedding of the current user input.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluation metrics - step 8 \n",
        "\n",
        "@dataclass\n",
        "class EvaluationMetrics:\n",
        "    # comprehensive evaluation metrics for the RAG system assessment. \n",
        "    \n",
        "    # Retrieval Quality Metrics\n",
        "    retrieval_relevance_score: float = 0.0\n",
        "    retrieval_diversity_score: float = 0.0\n",
        "    retrieval_coverage_score: float = 0.0\n",
        "    \n",
        "    # Response Quality Metrics\n",
        "    factual_accuracy_score: float = 0.0\n",
        "    coherence_score: float = 0.0\n",
        "    contextual_appropriateness_score: float = 0.0\n",
        "    hallucination_detection_score: float = 0.0\n",
        "    \n",
        "    # Memory Performance Metrics\n",
        "    memory_retention_score: float = 0.0\n",
        "    context_consistency_score: float = 0.0\n",
        "    long_term_memory_effectiveness: float = 0.0\n",
        "    \n",
        "    # System Performance Metrics\n",
        "    response_latency: float = 0.0\n",
        "    retrieval_latency: float = 0.0\n",
        "    total_tokens_used: int = 0\n",
        "    \n",
        "    # Timestamp and metadata\n",
        "    timestamp: datetime = None\n",
        "    conversation_turn: int = 0\n",
        "    user_query: str = \"\"\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        if self.timestamp is None:\n",
        "            self.timestamp = datetime.now()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conflict Resolution - Step 5 \n",
        "def resolve_context_conflicts(chat_history: list, retrieved_context: list) -> list:\n",
        "    # Resolve conflicts and redundancies using semantic understanding. \n",
        "    if not retrieved_context:\n",
        "        return [] \n",
        "    # Use Gemini to identify and resolve conflicts\n",
        "    clean_context = []\n",
        "    \n",
        "    for chunk in retrieved_context:\n",
        "        # Check if this chunk adds new information compared to current chat history\n",
        "        if not is_redundant_with_chat_history(chunk, chat_history):\n",
        "            clean_context.append(chunk)\n",
        "    return clean_context\n",
        "\n",
        "def is_redundant_with_chat_history(chunk: dict, chat_history: list) -> bool:\n",
        "    # Use Gemini to determine if chunk is redundant with current chat history. \n",
        "    if not chat_history:\n",
        "        return False\n",
        "    try:\n",
        "        # Get recent chat history (last 4 messages to avoid token bloat)\n",
        "        recent_history = chat_history[-4:] if len(chat_history) >= 4 else chat_history\n",
        "        recent_text = \" \".join([entry['parts'][0] for entry in recent_history])\n",
        "        \n",
        "        # Create prompt for Gemini to analyze redundancy\n",
        "        prompt = f\"\"\"\n",
        "        Analyze if this conversation chunk provides NEW information compared to recent chat history.\n",
        "        \n",
        "        RECENT CHAT HISTORY:\n",
        "        {recent_text}\n",
        "        \n",
        "        CONVERSATION CHUNK TO EVALUATE:\n",
        "        {chunk['content']}\n",
        "        \n",
        "        Consider:\n",
        "        1. Does the chunk introduce new topics not discussed recently?\n",
        "        2. Does it provide additional details on existing topics?\n",
        "        3. Does it offer different perspectives or solutions?\n",
        "        4. Is it a follow-up or expansion of recent discussions?\n",
        "        \n",
        "        Return ONLY: \"REDUNDANT\" if the chunk adds no new value, or \"NEW_INFO\" if it provides new information.\n",
        "        \"\"\"\n",
        "        # Use Gemini to evaluate (you'll need to pass the model instance)\n",
        "        response = model.generate_content(prompt)\n",
        "        result = response.text.strip().upper()\n",
        "        \n",
        "        return result == \"REDUNDANT\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in redundancy check: {e}\")\n",
        "        # Default to keeping the chunk if we can't evaluate\n",
        "        return False\n",
        "\n",
        "def resolve_semantic_conflicts(retrieved_context: list) -> list:\n",
        "    # Resolve conflicts between different retrieved chunks using semantic understanding. \n",
        "    if len(retrieved_context) <= 1:\n",
        "        return retrieved_context\n",
        "    \n",
        "    # Sort by relevance score\n",
        "    sorted_chunks = sorted(retrieved_context, key=lambda x: x['relevance_score'], reverse=True)\n",
        "    \n",
        "    # Use Gemini to identify and resolve conflicts\n",
        "    resolved_chunks = []\n",
        "    \n",
        "    for i, current_chunk in enumerate(sorted_chunks):\n",
        "        is_conflicting = False\n",
        "        \n",
        "        # Check against already accepted chunks\n",
        "        for accepted_chunk in resolved_chunks:\n",
        "            if has_semantic_conflict(current_chunk, accepted_chunk):\n",
        "                # Resolve conflict by keeping the more relevant one\n",
        "                if current_chunk['relevance_score'] > accepted_chunk['relevance_score']:\n",
        "                    # Replace the less relevant chunk\n",
        "                    resolved_chunks.remove(accepted_chunk)\n",
        "                    resolved_chunks.append(current_chunk)\n",
        "                is_conflicting = True\n",
        "                break\n",
        "        \n",
        "        if not is_conflicting:\n",
        "            resolved_chunks.append(current_chunk)\n",
        "    \n",
        "    return resolved_chunks\n",
        "\n",
        "def has_semantic_conflict(chunk1: dict, chunk2: dict) -> bool:\n",
        "    # Use Gemini to detect semantic conflicts between two chunks. \n",
        "    \n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "        Analyze if these two conversation chunks contain CONFLICTING information.\n",
        "        \n",
        "        CHUNK 1:\n",
        "        {chunk1['content']}\n",
        "        \n",
        "        CHUNK 2:\n",
        "        {chunk2['content']}\n",
        "        \n",
        "        Consider:\n",
        "        1. Do they contradict each other on facts, opinions, or solutions?\n",
        "        2. Do they provide different answers to the same question?\n",
        "        3. Do they have opposing viewpoints on the same topic?\n",
        "        4. Are they discussing the same subject but with conflicting information?\n",
        "        \n",
        "        Return ONLY: \"CONFLICT\" if there are contradictions, or \"NO_CONFLICT\" if they're compatible.\n",
        "        \"\"\"\n",
        "        \n",
        "        response = model.generate_content(prompt)\n",
        "        result = response.text.strip().upper()\n",
        "        \n",
        "        return result == \"CONFLICT\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in conflict detection: {e}\")\n",
        "        # Default to no conflict if we can't evaluate\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar2VBY-tFmer"
      },
      "outputs": [],
      "source": [
        "# @title Main Chat Functionality\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_enhanced_context_prompt_with_metadata(results: list, user_message: str, chat_history: list) -> str:\n",
        "    # Construct context prompt with intelligent conflict resolution and window management. \n",
        "    \n",
        "    if not results:\n",
        "        return \"\"\n",
        "    \n",
        "    # Resolve conflicts and redundancies using semantic understanding\n",
        "    clean_context = resolve_context_conflicts(chat_history, results)\n",
        "    \n",
        "    # Resolve conflicts between retrieved chunks\n",
        "    resolved_context = resolve_semantic_conflicts(clean_context)\n",
        "    \n",
        "    # Manage context window with semantic optimization\n",
        "    optimized_context = manage_context_window(chat_history, resolved_context, user_message)\n",
        "    \n",
        "    # Build the enhanced prompt\n",
        "    context_prompt = \"=== PREVIOUS RELEVANT CONVERSATIONS ===\\n\"\n",
        "    context_prompt += \"Use the following context to provide informed, contextual responses:\\n\\n\"\n",
        "    \n",
        "    # Sort by relevance score for better context ordering\n",
        "    sorted_results = sorted(optimized_context, key=lambda x: x['relevance_score'], reverse=True)\n",
        "    \n",
        "    for i, result in enumerate(sorted_results, 1):\n",
        "        context_prompt += f\"--- Conversation {i} (Relevance: {result['relevance_score']:.2f}) ---\\n\"\n",
        "        context_prompt += f\"Topic: {result['topic']}\\n\"\n",
        "        context_prompt += f\"Turn: {result['conversation_turn']}\\n\"\n",
        "        context_prompt += f\"Content: {result['content']}\\n\\n\"\n",
        "    \n",
        "    # Add current query analysis\n",
        "    context_prompt += \"=== CURRENT QUERY ANALYSIS ===\\n\"\n",
        "    context_prompt += f\"Query: {user_message}\\n\"\n",
        "    \n",
        "    # Identify related topics from retrieved context\n",
        "    if optimized_context:\n",
        "        topics = [r['topic'] for r in optimized_context]\n",
        "        context_prompt += f\"Related Topics: {', '.join(set(topics))}\\n\\n\"\n",
        "    \n",
        "    # Add specific instructions\n",
        "    context_prompt += \"=== RESPONSE GUIDELINES ===\\n\"\n",
        "    context_prompt += \"1. If the current query relates to previous topics, acknowledge the connection\\n\"\n",
        "    context_prompt += \"2. Build upon previous discussions when relevant\\n\"\n",
        "    context_prompt += \"3. Maintain consistency with previously provided information\\n\"\n",
        "    context_prompt += \"4. Reference specific previous conversations when helpful\\n\"\n",
        "    context_prompt += \"5. If you notice any conflicting information, clarify the discrepancy\\n\\n\"\n",
        "    \n",
        "    return context_prompt\n",
        "\n",
        "def manage_context_window(chat_history: list, retrieved_context: list, user_message: str, max_tokens: int = 4000) -> list:\n",
        "    # Manage context window with semantic understanding of content importance. \n",
        "    \n",
        "    # Estimate tokens (rough approximation: 1 word = 1.3 tokens but idk) \n",
        "    def estimate_tokens(text: str) -> int:\n",
        "        return int(len(text.split()) * 1.3)\n",
        "    \n",
        "    # Calculate available tokens\n",
        "    current_prompt = f\"User: {user_message}\\nAssistant:\"\n",
        "    available_tokens = max_tokens - estimate_tokens(current_prompt)\n",
        "    \n",
        "    # Start with most relevant context\n",
        "    sorted_context = sorted(retrieved_context, key=lambda x: x['relevance_score'], reverse=True)\n",
        "    \n",
        "    # Use Gemini to prioritize content when approaching token limits\n",
        "    if len(sorted_context) > 3:  # Only optimize if we have many chunks\n",
        "        sorted_context = optimize_context_for_tokens(sorted_context, available_tokens)\n",
        "    \n",
        "    return sorted_context\n",
        "\n",
        "def optimize_context_for_tokens(context_chunks: list, available_tokens: int) -> list:\n",
        "    # Use Gemini to intelligently select most valuable context within token limits.   \n",
        "    \n",
        "    try:\n",
        "        # Create a summary of all chunks for Gemini to evaluate\n",
        "        chunk_summaries = []\n",
        "        for i, chunk in enumerate(context_chunks):\n",
        "            summary = f\"Chunk {i+1}: {chunk['topic']} (Score: {chunk['relevance_score']:.2f})\"\n",
        "            chunk_summaries.append(summary)\n",
        "        \n",
        "        prompt = f\"\"\"\n",
        "        Given these conversation chunks, select the TOP 3 most valuable ones for context.\n",
        "        \n",
        "        AVAILABLE CHUNKS:\n",
        "        {chr(10).join(chunk_summaries)}\n",
        "        \n",
        "        Selection criteria:\n",
        "        1. Highest relevance to current conversation\n",
        "        2. Most unique and non-overlapping information\n",
        "        3. Best coverage of different topics/aspects\n",
        "        \n",
        "        Return ONLY the chunk numbers (e.g., \"1,3,5\") in order of priority.\n",
        "        \"\"\"\n",
        "        \n",
        "        response = model.generate_content(prompt)\n",
        "        selected_indices = [int(x.strip()) - 1 for x in response.text.split(',') if x.strip().isdigit()]\n",
        "        \n",
        "        # Return selected chunks\n",
        "        selected_chunks = [context_chunks[i] for i in selected_indices if 0 <= i < len(context_chunks)]\n",
        "        \n",
        "        return selected_chunks[:3]  # Ensure we don't exceed 3 chunks\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in context optimization: {e}\")\n",
        "        # Fallback to simple relevance-based selection\n",
        "        return context_chunks[:3]  \n",
        "\n",
        "def chat_with_gemini_with_memory():\n",
        "    print(\"Welcome to the Pseudo-infinite Chatbot! Type 'EXIT' to end the conversation.\")\n",
        "    \n",
        "    while True:\n",
        "        user_message = input(\"You: \")\n",
        "        if user_message.strip() == 'EXIT': \n",
        "            print(\"Chat ended.\")\n",
        "            break\n",
        "\n",
        "        # Get full results with metadata\n",
        "        results = vdb.query(user_message, k=3, chat_history=chat_history)\n",
        "        \n",
        "        # Construct enhanced context prompt\n",
        "        context_prompt = construct_enhanced_context_prompt_with_metadata(results, user_message)\n",
        "        \n",
        "        # Build the full prompt\n",
        "        full_prompt = f\"{context_prompt}=== CURRENT CONVERSATION ===\\n\"\n",
        "        for entry in chat_history:\n",
        "            role = \"User\" if entry[\"role\"] == \"user\" else \"Assistant\"\n",
        "            full_prompt += f\"{role}: {entry['parts'][0]}\\n\"\n",
        "        full_prompt += f\"User: {user_message}\\nAssistant:\"\n",
        "\n",
        "        try:\n",
        "            response = model.generate_content(\n",
        "                contents=[{\"role\": \"user\", \"parts\": [full_prompt]}]\n",
        "            )\n",
        "            gemini_response = response.candidates[0].content.parts[0].text\n",
        "            print(f\"Gemini: {gemini_response}\")\n",
        "\n",
        "            manage_chat_history(user_message, gemini_response)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            print(\"Please check your API key and ensure the model is accessible.\")\n",
        "\n",
        "# Note: Before running chat_with_gemini_with_memory(), ensure you have:\n",
        "# - Initialized your Gemini 'model' object (e.g., import google.generativeai as genai; model = genai.GenerativeModel('gemini-pro'))\n",
        "# - Set your Google API Key (genai.configure(api_key=\"YOUR_API_KEY\"))\n",
        "# - Run the SimulatedVectorDB class definition and the initial global variable setup (vdb, vdb_built_flag, vdb_index_counter, chat_history)\n",
        "# - Run the manage_chat_history function definition.\n",
        "\n",
        "# Example of how you would set up the globals and start the chat:\n",
        "# import google.generativeai as genai\n",
        "# import os\n",
        "# genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\")) # Or your actual key\n",
        "# model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "'''\n",
        "chat_history = []\n",
        "vdb = SimulatedVectorDB() # Re-initialize vdb if needed for a fresh chat session\n",
        "vdb_built_flag = False\n",
        "vdb_index_counter = 0 \n",
        "'''\n",
        "\n",
        "chat_with_gemini_with_memory()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
