{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhcX37ZAA03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pinecone\n",
            "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /Users/raunak/Documents/VS Code Folders/Stronghold Labs/.venv/lib/python3.12/site-packages (from pinecone) (2025.7.14)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/raunak/Documents/VS Code Folders/Stronghold Labs/.venv/lib/python3.12/site-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/raunak/Documents/VS Code Folders/Stronghold Labs/.venv/lib/python3.12/site-packages (from pinecone) (4.14.1)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /Users/raunak/Documents/VS Code Folders/Stronghold Labs/.venv/lib/python3.12/site-packages (from pinecone) (2.5.0)\n",
            "Collecting packaging<25.0,>=24.2 (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /Users/raunak/Documents/VS Code Folders/Stronghold Labs/.venv/lib/python3.12/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/raunak/Documents/VS Code Folders/Stronghold Labs/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/raunak/Documents/VS Code Folders/Stronghold Labs/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
            "Requirement already satisfied: six>=1.5 in /Users/raunak/Documents/VS Code Folders/Stronghold Labs/.venv/lib/python3.12/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Downloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, packaging, pinecone-plugin-assistant, pinecone\n",
            "\u001b[2K  Attempting uninstall: packaging\n",
            "\u001b[2K    Found existing installation: packaging 25.0\n",
            "\u001b[2K    Uninstalling packaging-25.0:\n",
            "\u001b[2K      Successfully uninstalled packaging-25.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pinecone]3/4\u001b[0m [pinecone]plugin-assistant]\n",
            "\u001b[1A\u001b[2KSuccessfully installed packaging-24.2 pinecone-7.3.0 pinecone-plugin-assistant-1.7.0 pinecone-plugin-interface-0.0.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/raunak/Documents/VS Code Folders/Stronghold Labs/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# @title Setup and Imports\n",
        "\n",
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "import os\n",
        "import pinecone\n",
        "\n",
        "# Configure your API key \n",
        "pineconeAPIKEY = \"pcsk_4xPCuD_6WLyNse1TcP3qmHKhMPKNdRCSQwU8g9MvVVVorLvLjWVboM3CwA76YnNzu8yd4V\" \n",
        "\n",
        "# Configure your API key \n",
        "APIKEY = \"AIzaSyA6bnQK33HYRspkrOi8-Q54bq4E4RgcHj4\" \n",
        "# It's recommended to store your API key securely, e.g., in environment variables\n",
        "# For this example, we'll assume it's directly set.\n",
        "# Replace \"YOUR_API_KEY\" with your actual Google API Key\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "genai.configure(api_key=APIKEY) \n",
        "\n",
        "# Initialize the Generative Model\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "# Chat history and truncation settings\n",
        "chat_history = []\n",
        "MAX_CHAT_HISTORY_LENGTH = 2 # Number of recent turns to keep in active memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaX88JunFB1a"
      },
      "outputs": [],
      "source": [
        "# Simulated Vector Database Class\n",
        "class SimulatedVectorDB:\n",
        "    def __init__(self):\n",
        "        self.items = []  # Stores (item_id, embedding, text_content) tuples\n",
        "        self._is_built = False # Internal flag to simulate index building state\n",
        "        self._embedding_dim = None # To store embedding dimension from first added item\n",
        "\n",
        "    def add_item(self, item_id: int, embedding: list, text_content: str):\n",
        "        \"\"\"Adds an item with its embedding and original text content to the VDB.\"\"\"\n",
        "        if not isinstance(embedding, list):\n",
        "            embedding = embedding.tolist() # Ensure numpy arrays are converted to list\n",
        "        self.items.append((item_id, embedding, text_content))\n",
        "        print(f\"DEBUG: Added item ID {item_id} to VDB (Text: '{text_content[:30]}...')\")\n",
        "\n",
        "\n",
        "    def build(self, n_trees: int):\n",
        "        \"\"\"Simulates building the VDB index. For this simple model, it just sets a flag.\"\"\"\n",
        "        if self._is_built:\n",
        "            raise RuntimeError(\"You can't build a built index. Call reset() first if you want to rebuild.\")\n",
        "\n",
        "        if not self.items:\n",
        "            print(\"WARNING: Building VDB on an empty set of items. Add items first.\")\n",
        "\n",
        "        # In a real VDB, this would optimize the search structure\n",
        "        print(f\"DEBUG: Simulated VDB building with {n_trees} trees. Index is now ready for efficient search.\")\n",
        "        self._is_built = True\n",
        "\n",
        "    def query(self, query_embedding: list, k: int = 1) -> list:\n",
        "        \"\"\"\n",
        "        Simulates querying the VDB. For simplicity, returns the k most recent items\n",
        "        or tries to find a specific ID if the query 'embedding' (here, we'll use a placeholder for actual query)\n",
        "        contains specific instructions (like 'ID: X').\n",
        "\n",
        "        In a real scenario, this would perform a similarity search.\n",
        "        Here, we'll implement a very basic \"retrieval by ID\" or \"latest items\" for demonstration.\n",
        "        \"\"\"\n",
        "        if not self._is_built:\n",
        "            # In a real VDB, query might fail or be inefficient if not built\n",
        "            print(\"WARNING: Querying VDB before it's built. Performance will be poor in a real system.\")\n",
        "\n",
        "        if not self.items: \n",
        "            return []\n",
        "\n",
        "        # Simple simulation: return the latest k items if no specific ID is requested\n",
        "        # For a true RAG, you'd calculate cosine similarity between query_embedding\n",
        "        # and all stored embeddings, then return the top-k most similar.\n",
        "\n",
        "        # To simulate finding by \"ID: X\" in user's example, we'll look for a string in query_embedding\n",
        "        # This is a hack for the 'Retrieved memory based on query (ID: 3)' prompt.\n",
        "        # A real query_embedding would be a list of floats.\n",
        "\n",
        "        # Let's just return the last k items added for now as a general \"retrieval\".\n",
        "        # A true \"query for ID X\" would be handled differently if the user wants specific ID retrieval.\n",
        "        retrieved_results = []\n",
        "        for i in range(1, min(k + 1, len(self.items) + 1)):\n",
        "            item_id, _, text_content = self.items[-i]\n",
        "            retrieved_results.append(f\"Retrieved content (ID: {item_id}): '{text_content}'\")\n",
        "\n",
        "        return retrieved_results\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the VDB, allowing it to be built again.\"\"\"\n",
        "        self.items = []\n",
        "        self._is_built = False\n",
        "        self._embedding_dim = None\n",
        "        print(\"DEBUG: VDB has been reset.\")\n",
        "\n",
        "\n",
        "# Global instance of our simulated VDB\n",
        "vdb = SimulatedVectorDB()\n",
        "# Global flag to ensure vdb.build() is called only once\n",
        "vdb_built_flag = False\n",
        "# Global counter for VDB item IDs\n",
        "vdb_index_counter = 0\n",
        "\n",
        "# (Assume get_embedding function is defined elsewhere, e.g., from a model)\n",
        "# Placeholder for get_embedding if it's not defined in the scope of execution\n",
        "def get_embedding(text: str) -> list:\n",
        "    \"\"\"Placeholder for an actual embedding generation function.\"\"\"\n",
        "    # In a real scenario, this would call a model to get a vector embedding\n",
        "    # For simulation, just return a dummy embedding based on text length or a hash\n",
        "    return [float(ord(c)) / 100 for c in text[:10]] # A dummy, simple embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxybJxbZFNA9"
      },
      "outputs": [],
      "source": [
        "# @title Chat History Management and Truncation\n",
        "# Assuming MAX_CHAT_HISTORY_LENGTH is defined elsewhere\n",
        "MAX_CHAT_HISTORY_LENGTH = 1 # For quick truncation testing\n",
        "\n",
        "# Initialize global chat_history (if not already done)\n",
        "chat_history = []\n",
        "\n",
        "\n",
        "def manage_chat_history(user_message, system_response):\n",
        "    global chat_history, vdb_index_counter, vdb_built_flag, vdb # Ensure vdb is global here\n",
        "\n",
        "    chat_history.append({\"role\": \"user\", \"parts\": [user_message]})\n",
        "    chat_history.append({\"role\": \"model\", \"parts\": [system_response]})\n",
        "\n",
        "    print(f\"\\n--- After adding new messages ---\")\n",
        "    print(f\"Current chat_history length: {len(chat_history)}\")\n",
        "\n",
        "    if len(chat_history) > MAX_CHAT_HISTORY_LENGTH * 2:\n",
        "        num_to_remove = len(chat_history) - MAX_CHAT_HISTORY_LENGTH * 2\n",
        "        messages_to_store = chat_history[:num_to_remove]\n",
        "        chat_history = chat_history[num_to_remove:]\n",
        "\n",
        "        print(f\"\\n--- Truncation initiated ---\")\n",
        "        print(f\"Number of messages to truncate: {num_to_remove}\")\n",
        "\n",
        "        # Add items to VDB\n",
        "        for message in messages_to_store:\n",
        "            text_content = message[\"parts\"][0]\n",
        "            embedding = get_embedding(text_content) # Assuming get_embedding is available\n",
        "            vdb.add_item(vdb_index_counter, embedding, text_content) # Pass text_content\n",
        "            vdb_index_counter += 1\n",
        "\n",
        "        # Only build the index ONCE, after the first batch of items is added\n",
        "        if not vdb_built_flag and vdb_index_counter > 0:\n",
        "            vdb.build(10) # Build the index with 10 trees (parameter is simulated)\n",
        "            vdb_built_flag = True # Set flag to True so it's not built again\n",
        "\n",
        "        print(f\"Truncated {num_to_remove} messages and stored in VDB.\")\n",
        "        print(f\"New chat_history length after truncation: {len(chat_history)}\")\n",
        "        print(f\"Total items in VDB: {vdb_index_counter}\")\n",
        "\n",
        "# --- Test the functionality ---\n",
        "# Reset VDB for a clean test run\n",
        "vdb.reset()\n",
        "vdb_built_flag = False\n",
        "vdb_index_counter = 0\n",
        "chat_history = [] # Reset chat history too for a clean start\n",
        "\n",
        "print(\"--- Initializing for test ---\")\n",
        "\n",
        "# Simulate some conversation\n",
        "manage_chat_history(\"Hello, how are you?\", \"I'm doing well, thank you!\")\n",
        "manage_chat_history(\"What is the capital of France?\", \"Paris is the capital of France.\")\n",
        "\n",
        "# This should trigger truncation and VDB storage\n",
        "manage_chat_history(\"Can you tell me more about AI?\", \"AI is a rapidly evolving field.\")\n",
        "\n",
        "# Simulate retrieval from VDB\n",
        "print(f\"\\n--- Retrieval from VDB ---\")\n",
        "# In a real RAG, you'd query with an embedding of the current user input.\n",
        "# Here, we'll just demonstrate retrieving the latest few items from the VDB for completeness.\n",
        "query_results = vdb.query(get_embedding(\"dummy query for retrieval\"), k=2) # k=2 to get more than one\n",
        "if query_results:\n",
        "    for result in query_results:\n",
        "        print(result)\n",
        "else:\n",
        "    print(\"VDB is empty. No retrieval.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar2VBY-tFmer"
      },
      "outputs": [],
      "source": [
        "# @title Main Chat Functionality\n",
        "\n",
        "# Define retrieve_from_vdb function\n",
        "def retrieve_from_vdb(query_text: str) -> list:\n",
        "    \"\"\"\n",
        "    Retrieves relevant context from the VDB based on the query text.\n",
        "    Returns a list of strings, where each string is the content of a retrieved memory.\n",
        "    \"\"\"\n",
        "    global vdb # Ensure vdb is accessible\n",
        "\n",
        "    if not vdb._is_built and vdb.items:\n",
        "        print(\"WARNING: Querying VDB before it's built. Results may not be optimal.\")\n",
        "\n",
        "    if not vdb.items:\n",
        "        print(\"--- VDB is empty. No retrieval. ---\")\n",
        "        return []\n",
        "\n",
        "    query_embedding = get_embedding(query_text)\n",
        "    # The vdb.query method now returns formatted strings, so we need to parse them.\n",
        "    raw_retrieved_items = vdb.query(query_embedding, k=2) # Retrieve top 2 items\n",
        "    \n",
        "    extracted_contexts = []\n",
        "    if raw_retrieved_items:\n",
        "        print(\"--- Retrieval from VDB ---\")\n",
        "        print(f\"Query for retrieval: '{query_text}'\")\n",
        "        for item_str in raw_retrieved_items:\n",
        "            # Example format from vdb.query: \"Retrieved content (ID: 1): 'Hello, how are you?'\"\n",
        "            # We want to extract just 'Hello, how are you?'\n",
        "            parts = item_str.split(\"': '\")\n",
        "            if len(parts) > 1:\n",
        "                text_content = parts[1].rstrip(\"'\") # Get the text part and remove trailing single quote\n",
        "                extracted_contexts.append(text_content)\n",
        "            else:\n",
        "                extracted_contexts.append(item_str) # Fallback if format is unexpected\n",
        "\n",
        "    # Print the actual content retrieved\n",
        "    if extracted_contexts:\n",
        "        print(\"Retrieved content:\")\n",
        "        for context in extracted_contexts:\n",
        "            print(f\"- '{context}'\")\n",
        "\n",
        "    return extracted_contexts\n",
        "\n",
        "def chat_with_gemini_with_memory():\n",
        "    print(\"Welcome to the Pseudo-infinite Chatbot! Type 'exit' to end the conversation.\")\n",
        "    # Assuming 'model' is defined and initialized elsewhere (e.g., gemini-pro)\n",
        "    # Assuming 'chat_history' is globally initialized as an empty list\n",
        "\n",
        "    while True:\n",
        "        user_message = input(\"You: \")\n",
        "        if user_message.lower() == 'exit':\n",
        "            print(\"Chat ended.\")\n",
        "            break\n",
        "\n",
        "        # Retrieve relevant context from VDB\n",
        "        # This function will now correctly return a list of text strings\n",
        "        retrieved_context = retrieve_from_vdb(user_message)\n",
        "        context_prompt = \"\"\n",
        "        if retrieved_context:\n",
        "            context_prompt = \"The user has previously discussed the following:\\n\" + \"\\n\".join(retrieved_context) + \"\\n\"\n",
        "\n",
        "        # Construct the full prompt for the LLM\n",
        "        # Prepend the retrieved context to the system prompt\n",
        "        full_prompt = f\"{context_prompt}Current conversation:\\n\"\n",
        "        for entry in chat_history:\n",
        "            role = \"User\" if entry[\"role\"] == \"user\" else \"Model\"\n",
        "            full_prompt += f\"{role}: {entry['parts'][0]}\\n\"\n",
        "        full_prompt += f\"User: {user_message}\\nModel:\"\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Generate Gemini's response\n",
        "            # Ensure 'model' is properly initialized (e.g., genai.GenerativeModel('gemini-pro'))\n",
        "            response = model.generate_content(\n",
        "                contents=[{\"role\": \"user\", \"parts\": [full_prompt]}]\n",
        "            )\n",
        "            gemini_response = response.candidates[0].content.parts[0].text\n",
        "            print(f\"Gemini: {gemini_response}\")\n",
        "\n",
        "            # Manage chat history (truncate and store in VDB if needed)\n",
        "            manage_chat_history(user_message, gemini_response)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            print(\"Please check your API key and ensure the model is accessible.\")\n",
        "\n",
        "# Note: Before running chat_with_gemini_with_memory(), ensure you have:\n",
        "# - Initialized your Gemini 'model' object (e.g., import google.generativeai as genai; model = genai.GenerativeModel('gemini-pro'))\n",
        "# - Set your Google API Key (genai.configure(api_key=\"YOUR_API_KEY\"))\n",
        "# - Run the SimulatedVectorDB class definition and the initial global variable setup (vdb, vdb_built_flag, vdb_index_counter, chat_history)\n",
        "# - Run the manage_chat_history function definition.\n",
        "\n",
        "# Example of how you would set up the globals and start the chat:\n",
        "# import google.generativeai as genai\n",
        "# import os\n",
        "# genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\")) # Or your actual key\n",
        "# model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "chat_history = []\n",
        "vdb = SimulatedVectorDB() # Re-initialize vdb if needed for a fresh chat session\n",
        "vdb_built_flag = False\n",
        "vdb_index_counter = 0 \n",
        "\n",
        "chat_with_gemini_with_memory()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
